Following the examples in this guide requires access to the Llama 2 models in the HuggingFace Hub, as well as hardware with GPU resources.
## 2.1. Getting access to Llama 2 models on the HuggingFace hub

The Llama 2 models are governed by the Meta license. In order to download the model weights and tokenizer from the HuggingFace Hub, an account needs to receive authorization from Meta. You can do this by following these steps:

1. Create an account on HuggingFace
2. Request access to Llama 2 from Meta (Make sure to use the same email as your HF account): [https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)
3. Request access to the appropriate Llama 2 model on HF: [https://huggingface.co/meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)

Keep in mind requests can take 1-2 days to be processed.

## 2.2. Using local hardware

Both inference and fine-tuning require access to hardware with GPU if done locally. The examples in this guide require access to a GPU with at least 16 GB VRAM for inference, and 16-40 GB for fine-tuning, depending on the format you wish to use for saving your fine-tuned model.

For your dev environment I’d recommend you install [miniconda](https://docs.conda.io/projects/miniconda/en/latest/ "https://docs.conda.io/projects/miniconda/en/latest/"), and install any individual packages as required.

To install the latest version of miniconda:

1. Download miniconda installer:
`wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh`
Installers for previous versions can be found here: [https://docs.conda.io/en/latest/miniconda.html](https://docs.conda.io/en/latest/miniconda.html "https://docs.conda.io/en/latest/miniconda.html")
2. Execute installer:
`bash Miniconda3-latest-Linux-x86_64.sh`

3. Follow the instructions on the screen.

In terms of how to work with your GPU server, you have a few options. You can code locally on your computer, then manually move any code and data to vili when you need to run something with GPU, but this can quickly become cumbersome. Some better options are:

- If using an IDE like PyCharm, you can setup a project with a remote ssh interpreter ad run it on vili: [https://www.jetbrains.com/help/pycharm/configuring-remote-interpreters-via-ssh.html](https://www.jetbrains.com/help/pycharm/configuring-remote-interpreters-via-ssh.html)
- You can run Jupyter Notebook on a vili as remote server and then access from you pc: [https://docs.anaconda.com/free/anaconda/jupyter-notebooks/remote-jupyter-notebook/](https://docs.anaconda.com/free/anaconda/jupyter-notebooks/remote-jupyter-notebook/)

## 2.3. Using Google Colab

An alternative is to use [Google Colab](https://research.google.com/colaboratory/). This is a cloud-based platform offered by Google Research that allows users to write and execute Python code through the browser. The free version includes access to a T4 GPU with 16 GB of VRAM. This is enough for inference with the 7 billion version of Llama, but you'll likely run into memory issues for fine-tuning. Another limitation of  the free version is lack of background execution.  The paid version allows for this and also includes access to more powerful GPUs.
