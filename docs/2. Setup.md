Following the examples in this guide requires access to the Llama 2 models in the HuggingFace Hub, as well as hardware with GPU resources. 
## 2.1. Getting access to Llama 2 models on the HuggingFace hub

The Llama 2 models are governed by the Meta license. In order to download the model weights and tokenizer from the HuggingFace Hub, an account needs to receive authorization from Meta. You can do this by following these steps:

1. Create an account on HuggingFace
2. Request access to Llama 2 from Meta (Make sure to use the same email as your HF account): [https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)
3. Request access to the appropriate Llama 2 model on HF: [https://huggingface.co/meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)

Keep in mind requests can take 1-2 days to be processed.

## 2.2. Using Vili

Both inference and fine-tuning require access to hardware with GPU if done locally. You have been granted access to such hardware on the `vili` server. This server contains 8 Nvidia A6000 GPUs with 48GB of VRAM each. This is more than enough for inference with the smaller variants of Llama 2, and for fine-tuning the 7 billion parameter variant.

Connection to `vili` is done through `ssh` from the command line. If using a mac computer this can be done with the command:

`ssh vili.inf.susx.ac.uk`

There should be an equivalent for Windows. Your username and password should be the same as for your university account (`<username>@sussex.ac.uk`). This server is not externally exposed, so you'll need to do this while connected to the university network. Otherwise, jump in through a log-in node, ie. ssh to `unix.susx.ac.uk` first, then ssh again to `vili.inf.susx.ac.uk` from there.

Users will also be granted access to the storage server `thetis2`. This can be accessed from vili through a command like:

`cd /srv/thetis2`

For your dev environment I’d recommend you install [miniconda](https://docs.conda.io/projects/miniconda/en/latest/ "https://docs.conda.io/projects/miniconda/en/latest/"), and install any individual packages as required. Any installation and storage of data, models, etc, should be done on `thetis2`, since storage directly on `vili` is limited. Once you have access to `thetis2` a you need to create a personal directory there (unless it already exists): `/srv/thetis2/<username>`.  In general, avoid storing data/installing on the home directory (i.e. `~/`).

To install the latest version of miniconda on `vili`:

1. Download miniconda installer:
`wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh`
Installers for previous versions can be found here: [https://docs.conda.io/en/latest/miniconda.html](https://docs.conda.io/en/latest/miniconda.html "https://docs.conda.io/en/latest/miniconda.html")
2. Execute installer:
`bash Miniconda3-latest-Linux-x86_64.sh`

3. Follow the instructions on the screen. When asked for the install location, make sure to use your directory on the storage drive (`/srv/thetis2/<username>`)

In terms of how to work on the server, you have a few options. You can code locally on your computer, then manually move any code and data to vili when you need to run something with GPU, but this can quickly become cumbersome. Some better options are:

- If using an IDE like PyCharm, you can setup a project with a remote ssh interpreter ad run it on vili: [https://www.jetbrains.com/help/pycharm/configuring-remote-interpreters-via-ssh.html](https://www.jetbrains.com/help/pycharm/configuring-remote-interpreters-via-ssh.html)
- You can run Jupyter Notebook on a vili as remote server and then access from you pc: [https://docs.anaconda.com/free/anaconda/jupyter-notebooks/remote-jupyter-notebook/](https://docs.anaconda.com/free/anaconda/jupyter-notebooks/remote-jupyter-notebook/)

## 2.3. Using Google Colab

An alternative is to use [Google Colab](https://research.google.com/colaboratory/). This is a cloud-based platform offered by Google Research that allows users to write and execute Python code through the browser. The free version includes access to a T4 GPU with 16 GB of VRAM. This is enough for inference with the 7 billion version of Llama, but you'll likely run into memory issues for fine-tuning. Another limitation of  the free version is lack of background execution.  The paid version allows for this and also includes access to more powerful GPUs. 