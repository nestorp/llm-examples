
Large Language Models (LLMs) are deep learning models with a vast number of parameters, often trained on extensive text corpora. They excel in understanding and generating human-like text, capturing the nuances of language in a way that smaller models cannot. Some notable examples include OpenAI's GPT-3 and GPT-4, and Meta's Llama. 

LLMs can be used  across a large variety of domains of domains: from text generation, translation, summarisation, to sentiment analysis and beyond. Utilising LLMs generally involves either inference or fine-tuning. Inference refers to leveraging the pre-trained models to perform tasks like text generation or classification. Fine-tuning, on the other hand, implies adapting these models to specific tasks or domains by training them further on a smaller, domain-specific dataset.

This guide aims to provide an introduction on how to apply LLMs for your tasks and explore their capabilities. For this, we'll use models from Meta's Llama 2 family. 

## 1.1. Llama 2

Meta's Llama 2 family of models is a collection of large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters, which have been pre-trained and fine-tuned for various tasks. Notably, these models are open-access, with Meta releasing the code and model weights, allowing for local use and further fine-tuning of the models.

Llama 2 models are available through Meta's site, as well as the HuggingFace library, though access requires authorisation from Meta. Details on how to acquire this authorisation are presented in section [[2. Setup]]. Two main variants of the models are available, a pre-trained base model, and a version fine-tuned for chat applications.

The examples on this guide use the smallest model variant, with 7 billion parameters, fine-tuned for chat usage. This version can be run and fine-tuned on modest GPU resources, and allows for quick experimentation and prototyping. We will also take advantage of several techniques for resource-efficient inference and fine-tuning. 
## 1.2. HuggingFace

The examples in this guide all use libraries published by HuggingFace. HuggingFace is a company that develops open-source tools and resources to build, train and deploy machine learning models. The examples in this guide make use of several of their libraries, which are by now industry standard. 

[**Transformers**](https://huggingface.co/docs/transformers/index)
The Transformers library contains open-source implementations of transformer models for text, image, and audio tasks.

[**Datasets**](https://huggingface.co/docs/datasets/index)
Datasets is a library for  accessing and sharing datasets for Audio, Computer Vision, and Natural Language Processing (NLP) tasks.

[**TRL**](https://huggingface.co/docs/trl/index)
TRL is a library that provides a set of tools to train generative language models with reinforcement learning, including the steps of supervised fine-tuning and reward modelling.

## 1.3. Additional Resources
[Llama 2 is here - get it on Hugging Face](https://huggingface.co/blog/llama2), the Llama 2 introductory blog post on Hugging Face.
[Llama 2 documentation on Hugging Face](https://huggingface.co/docs/transformers/main/model_doc/llama2), detailed Llama 2 documentation, including links to other useful resources.