{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LLM guide","text":"<p>Examples on using open access LLMs for inference and fine-tuning. </p> <p>Start here: 1.Introduction!</p>"},{"location":"1.%20Introduction/","title":"1. Introduction","text":"<p>Large Language Models (LLMs) are deep learning models with a vast number of parameters, often trained on extensive text corpora. They excel in understanding and generating human-like text, capturing the nuances of language in a way that smaller models cannot. Some notable examples include OpenAI's GPT-3 and GPT-4, and Meta's Llama. </p> <p>LLMs can be used  across a large variety of domains of domains: from text generation, translation, summarisation, to sentiment analysis and beyond. Utilising LLMs generally involves either inference or fine-tuning. Inference refers to leveraging the pre-trained models to perform tasks like text generation or classification. Fine-tuning, on the other hand, implies adapting these models to specific tasks or domains by training them further on a smaller, domain-specific dataset.</p> <p>This guide aims to provide an introduction on how to apply LLMs for your tasks and explore their capabilities. For this, we'll use models from Meta's Llama 2 family. </p>"},{"location":"1.%20Introduction/#11-llama-2","title":"1.1. Llama 2","text":"<p>Meta's Llama 2 family of models is a collection of large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters, which have been pre-trained and fine-tuned for various tasks. Notably, these models are open-access, with Meta releasing the code and model weights, allowing for local use and further fine-tuning of the models.</p> <p>Llama 2 models are available through Meta's site, as well as the HuggingFace library, though access requires authorisation from Meta. Details on how to acquire this authorisation are presented in section 2. Setup. Two main variants of the models are available, a pre-trained base model, and a version fine-tuned for chat applications.</p> <p>The examples on this guide use the smallest model variant, with 7 billion parameters, fine-tuned for chat usage. This version can be run and fine-tuned on modest GPU resources, and allows for quick experimentation and prototyping. We will also take advantage of several techniques for resource-efficient inference and fine-tuning. </p>"},{"location":"1.%20Introduction/#12-huggingface","title":"1.2. HuggingFace","text":"<p>The examples in this guide all use libraries published by HuggingFace. HuggingFace is a company that develops open-source tools and resources to build, train and deploy machine learning models. The examples in this guide make use of several of their libraries, which are by now industry standard. </p> <p>Transformers The Transformers library contains open-source implementations of transformer models for text, image, and audio tasks.</p> <p>Datasets Datasets is a library for  accessing and sharing datasets for Audio, Computer Vision, and Natural Language Processing (NLP) tasks.</p> <p>TRL TRL is a library that provides a set of tools to train generative language models with reinforcement learning, including the steps of supervised fine-tuning and reward modelling.</p>"},{"location":"1.%20Introduction/#13-additional-resources","title":"1.3. Additional Resources","text":"<p>Llama 2 is here - get it on Hugging Face, the Llama 2 introductory blog post on Hugging Face. Llama 2 documentation on Hugging Face, detailed Llama 2 documentation, including links to other useful resources.</p>"},{"location":"2.%20Setup/","title":"2. Setup","text":"<p>Following the examples in this guide requires access to the Llama 2 models in the HuggingFace Hub, as well as hardware with GPU resources.</p>"},{"location":"2.%20Setup/#21-getting-access-to-llama-2-models-on-the-huggingface-hub","title":"2.1. Getting access to Llama 2 models on the HuggingFace hub","text":"<p>The Llama 2 models are governed by the Meta license. In order to download the model weights and tokenizer from the HuggingFace Hub, an account needs to receive authorization from Meta. You can do this by following these steps:</p> <ol> <li>Create an account on HuggingFace</li> <li>Request access to Llama 2 from Meta (Make sure to use the same email as your HF account): https://ai.meta.com/resources/models-and-libraries/llama-downloads/</li> <li>Request access to the appropriate Llama 2 model on HF: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf</li> </ol> <p>Keep in mind requests can take 1-2 days to be processed.</p>"},{"location":"2.%20Setup/#22-using-local-hardware","title":"2.2. Using local hardware","text":"<p>Both inference and fine-tuning require access to hardware with GPU if done locally. The examples in this guide require access to a GPU with at least 16 GB VRAM for inference, and 16-40 GB for fine-tuning, depending on the format you wish to use for saving your fine-tuned model.</p> <p>For your dev environment I\u2019d recommend you install\u00a0miniconda, and install any individual packages as required.</p> <p>To install the latest version of miniconda:</p> <ol> <li>Download miniconda installer: <code>wget\u00a0https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh</code> Installers for previous versions can be found here:\u00a0https://docs.conda.io/en/latest/miniconda.html</li> <li> <p>Execute installer: <code>bash Miniconda3-latest-Linux-x86_64.sh</code></p> </li> <li> <p>Follow the instructions on the screen.</p> </li> </ol> <p>In terms of how to work with your GPU server, you have a few options. You can code locally on your computer, then manually move any code and data to the server when you need to run something on the GPU, but this can quickly become cumbersome. Some better options are:</p> <ul> <li>If using an IDE like PyCharm, you can setup a project with a remote ssh interpreter and run it on your GPU server:\u00a0Configure an interpreter using SSH</li> <li>You can run Jupyter Notebook on a GPU server as remote server and then access from your computer:\u00a0Running Jupyter Notebook on a remote server</li> </ul>"},{"location":"2.%20Setup/#23-using-google-colab","title":"2.3. Using Google Colab","text":"<p>An alternative is to use Google Colab. This is a cloud-based platform offered by Google Research that allows users to write and execute Python code through the browser. The free version includes access to a T4 GPU with 16 GB of VRAM. This is enough for inference with the 7 billion version of Llama, but you'll likely run into memory issues for fine-tuning. Another limitation of  the free version is lack of background execution.  The paid version allows for this and also includes access to more powerful GPUs.</p>"},{"location":"3.%20Inference/","title":"3. Inference","text":"<p>For many applications, generative LLMs can be applied directly through inference with no fine-tuning. In these cases, a clear formulation of the task is necessary, which is then communicated to the model through prompting. </p> <p>In order to use Llama 2 for inference, you'll need to have been granted access to the model both on Meta's platform and the HuggingFace Hub. The steps to acquire access are described in Section 2. Setup. </p>"},{"location":"3.%20Inference/#31-sentiment-analysis-with-llama-2","title":"3.1. Sentiment analysis with Llama 2","text":"<p>The following notebook contains a demonstration of using Llama 2 to tackle a sentiment analysis task, using the IMDB dataset. The IMDB dataset is a widely-used resource in the field of Natural Language Processing (NLP) for sentiment analysis tasks. It contains 50,000 reviews from the Internet Movie Database (IMDB) that are labeled as either positive or negative, and thus a binary classification task.</p> <p>The challenge in using a chat model for this task relies on ensuring that its outputs adhere to a binary format that can be used for automatic evaluation. This can be achieved through appropriate use of prompts. The notebook blow includes an example of how to prompt Llama 2 in order to have the model generate answers that stick to this binary format.</p> <p>Prompts and Special Tokens</p> <p>LLMs trained for chat are often trained to make use a system of different types of prompts that provide instructions that affect the model's outputs in different ways. This usually involves the use of a 'System Prompt', which affects the entirety of the conversation with the model and can be useful to influence aspects such as the model's writing style; and 'User Prompts', which are the instructions received from the user and receive replies from the model. The Llama 2 chat models have been trained with a set of special tokens that delimit the portions of the input that correspond to each of these prompt types. The template looks like this:</p> <pre><code>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\n{{ system_prompt }}\n&lt;&lt;/SYS&gt;&gt;\n\n{{ user_message }} [/INST]\n</code></pre> <p>It's important to use these special tokens in order to take advantage of the models' pre-training. This can have a significant influence on performance. </p> <p>Example Notebook</p> <p>Inference with Transformers and Llama 2</p>"},{"location":"3.%20Inference/#32-additional-resources","title":"3.2. Additional resources","text":"<ul> <li>IMFB Dataset on Hugging Face Hub, the dataset used in this example.</li> <li>Llama-2-7b-chat-hf on Hugging Face Hub, the model used in this example.</li> </ul>"},{"location":"4.%20Fine-tuning/","title":"4. Fine tuning","text":"<p>Sometime, the task we wish the model to perform is complicated, or we want to have the best performance possible. In these cases, fine-tuning the language model can result in improved performance.</p> <p>As with inference, in order to use Llama 2 for fine-tuning, you'll need to have been granted access to the model both on Meta's platform and the HuggingFace Hub. The steps to acquire access are described in Section 2. Setup. </p>"},{"location":"4.%20Fine-tuning/#41-fine-tuning-llama-2-for-news-summarisation","title":"4.1. Fine-tuning Llama 2 for News Summarisation","text":"<p>The following notebook contains an example of supervised fine-tuning Llama 7b chat for a news summarisation task, using the CNN / DailyMail dataset. This dataset is an English-language dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail. The current version supports both extractive and abstractive summarisation, though the original version was created for machine reading and comprehension and abstractive question answering.</p> <p>Llama 7b is actually quite capable of producing summaries of these news articles without fine-tuning. However, the format it produces for these summaries does not exactly follow the one used in the dataset. You may run into a similar situation where you need the model to stick to a specific format in its output, and are unable to get it to do so through prompting only. This is when fine-tuning becomes necessary.</p> <p>Since we are fine-tuning the Llama 2 chat model variant, we still need to use the prompt template with the special tokens described in Section 3. Inference. To simplify this, an adapted version of the CNN / DailyMail dataset that has been created that has been modified to match this template. We also train on a sample of only 10% of the data, to keep training times reasonable. </p> <p>Example Notebook</p> <p>Fine-tuning Llama 2 with Transformers and TRL</p>"},{"location":"4.%20Fine-tuning/#42-additional-resources","title":"4.2. Additional Resources","text":"<ul> <li>TRL Documentation, the library we are using for fine-tuning.</li> <li>Adapted CNN / DailyMail Dataset, our training dataset.</li> <li>Llama-2-7b-chat-hf on Hugging Face Hub, the model fine-tuned in the example notebook.</li> <li>Illustrating Reinforcement Learning from Human Feedback (RLHF), a primer on Reinforcement Learning from Human Feedback, the approach commonly used to train chat models.</li> <li>Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA, more information on quantization and LoRA, techniques for resource-efficient inference and fine-tuning.</li> <li>LLaMA 2 - Every Resource you need, a compilation of relevant resources to learn about LLaMA 2 and how to get started quickly.</li> </ul>"},{"location":"5.%20Inference%20with%20OpenAI/","title":"5. Inference with OpenAI","text":"<p>While open access models such as Llama 2 are making great progress, they still lag behind in performance when compared with models created by OpenAI, such as GPT3.5 and GPT4. This may be due to raw model size, better data, or model architecture. Very few details about OpenAI's models are public, making this hard to determine. </p>"},{"location":"5.%20Inference%20with%20OpenAI/#51-sentiment-analysis-with-openai","title":"5.1. Sentiment Analysis with OpenAI","text":"<p>Automated access to OpenAI's model is available only through their paid API.  This example uses the same task and dataset as in Section 3. Inference. You can create an account on their page, but keep in mind that they charge by the token, which can quickly add up with large datasets. </p> <p>Adapting GPT models at tasks such as classification also relies on appropriate use of prompting. Fortunately, GPT models in general are quite good at following instructions and sticking to a specific output format, as long as this is clearly expressed. The example notebook below shows an example of prompting the GPT3.5 model to perform sentiment analysis on a sample of the IMDB dataset.</p> <p>Example Notebook</p> <p>Inference with OpenAI API</p>"},{"location":"5.%20Inference%20with%20OpenAI/#52-resources","title":"5.2. Resources","text":"<ul> <li>OpenAI API Documentation, documentation for the OpenAI API used in the example notebook</li> </ul>"}]}